{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tune SmolVLM2 2b with LoRA","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade accelerate datasets peft bitsandbytes pyav num2words","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:08:12.666626Z","iopub.execute_input":"2025-03-01T15:08:12.666880Z","iopub.status.idle":"2025-03-01T15:08:16.583399Z","shell.execute_reply.started":"2025-03-01T15:08:12.666857Z","shell.execute_reply":"2025-03-01T15:08:16.582456Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:08:16.590178Z","iopub.execute_input":"2025-03-01T15:08:16.590465Z","iopub.status.idle":"2025-03-01T15:08:34.686111Z","shell.execute_reply.started":"2025-03-01T15:08:16.590440Z","shell.execute_reply":"2025-03-01T15:08:34.684973Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wandb_key)\nwandb.init(project=\"kaggle\", name=\"smolvlm2b_finetune_lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:08:34.687142Z","iopub.execute_input":"2025-03-01T15:08:34.687403Z","iopub.status.idle":"2025-03-01T15:08:47.332616Z","shell.execute_reply.started":"2025-03-01T15:08:34.687381Z","shell.execute_reply":"2025-03-01T15:08:47.331530Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhmzbo\u001b[0m (\u001b[33mhmzbo-vektrai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250301_150841-9evtzwop</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hmzbo-vektrai/kaggle/runs/9evtzwop' target=\"_blank\">smolvlm2b_finetune_lora</a></strong> to <a href='https://wandb.ai/hmzbo-vektrai/kaggle' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hmzbo-vektrai/kaggle' target=\"_blank\">https://wandb.ai/hmzbo-vektrai/kaggle</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hmzbo-vektrai/kaggle/runs/9evtzwop' target=\"_blank\">https://wandb.ai/hmzbo-vektrai/kaggle/runs/9evtzwop</a>"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hmzbo-vektrai/kaggle/runs/9evtzwop?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7c9ec7778df0>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom transformers import AutoProcessor, BitsAndBytesConfig, AutoModelForImageTextToText\nimport os\n\n\nUSE_LORA = True\nUSE_QLORA = False\nSMOL = False\n\nmodel_id = \"HuggingFaceTB/SmolVLM2-500M-Video-Instruct\" if SMOL else \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n\nprocessor = AutoProcessor.from_pretrained(\n    model_id\n)\n\nif USE_QLORA or USE_LORA:\n    lora_config = LoraConfig(\n        r=8,\n        lora_alpha=8,\n        lora_dropout=0.1,\n        target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n        use_dora=False if USE_QLORA else True,\n        init_lora_weights=\"gaussian\"\n    )\n    lora_config.inference_mode = False\n    if USE_QLORA:\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16\n        )\n\n    model = AutoModelForImageTextToText.from_pretrained(\n        model_id,\n        quantization_config=bnb_config if USE_QLORA else None,\n        #_attn_implementation=\"flash_attention_2\",\n        device_map=\"auto\"\n    )\n    model.add_adapter(lora_config)\n    model.enable_adapters()\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, lora_config)\n    print(model.get_nb_trainable_parameters())\nelse:\n    model = AutoModelForImageTextToText.from_pretrained(\n        model_id,\n        torch_dtype=torch.bfloat16,\n        #_attn_implementation=\"flash_attention_2\",\n    ).to(\"cuda\")\n\n    # if you'd like to only fine-tune LLM\n    for param in model.model.vision_model.parameters():\n        param.requires_grad = False\n\npeak_mem = torch.cuda.max_memory_allocated()\nprint(f\"The model as is is holding: {peak_mem / 1024**3:.2f} of GPU RAM\")","metadata":{"execution":{"iopub.status.busy":"2025-03-01T15:08:47.347668Z","iopub.execute_input":"2025-03-01T15:08:47.347921Z","iopub.status.idle":"2025-03-01T15:09:01.667875Z","shell.execute_reply.started":"2025-03-01T15:08:47.347892Z","shell.execute_reply":"2025-03-01T15:09:01.667029Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ff369837b94667a54af8869f37510c"}},"metadata":{}},{"name":"stdout","text":"(11269248, 2258054128)\nThe model as is is holding: 7.99 of GPU RAM\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"model = model.to(dtype=torch.bfloat16)  # Explicitly set dtype","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:10:13.178439Z","iopub.execute_input":"2025-03-01T15:10:13.178778Z","iopub.status.idle":"2025-03-01T15:10:13.226883Z","shell.execute_reply.started":"2025-03-01T15:10:13.178754Z","shell.execute_reply":"2025-03-01T15:10:13.226248Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"TIGER-Lab/VideoFeedback\", \"real\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:10:24.552950Z","iopub.execute_input":"2025-03-01T15:10:24.553261Z","iopub.status.idle":"2025-03-01T15:10:26.192281Z","shell.execute_reply.started":"2025-03-01T15:10:24.553237Z","shell.execute_reply":"2025-03-01T15:10:26.191506Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"split_ds = ds[\"train\"].train_test_split(test_size=0.5)\ntrain_ds = split_ds[\"train\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:10:26.193421Z","iopub.execute_input":"2025-03-01T15:10:26.194028Z","iopub.status.idle":"2025-03-01T15:10:26.210817Z","shell.execute_reply.started":"2025-03-01T15:10:26.194001Z","shell.execute_reply":"2025-03-01T15:10:26.209957Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"del split_ds, ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:10:26.212212Z","iopub.execute_input":"2025-03-01T15:10:26.212462Z","iopub.status.idle":"2025-03-01T15:10:26.216711Z","shell.execute_reply.started":"2025-03-01T15:10:26.212440Z","shell.execute_reply":"2025-03-01T15:10:26.216085Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(f\"prompt:  {train_ds[0]['text prompt']}, video: {train_ds[0]['video link']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:10:26.217497Z","iopub.execute_input":"2025-03-01T15:10:26.217792Z","iopub.status.idle":"2025-03-01T15:10:26.231973Z","shell.execute_reply.started":"2025-03-01T15:10:26.217770Z","shell.execute_reply":"2025-03-01T15:10:26.231247Z"}},"outputs":[{"name":"stdout","text":"prompt:  we zoom in on the odd figure, video: https://huggingface.co/datasets/hexuan21/VideoFeedback-videos-mp4/resolve/main/p/p009305.mp4\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\nimage_token_id = processor.tokenizer.additional_special_tokens_ids[\n    processor.tokenizer.additional_special_tokens.index(\"<image>\")\n]\n\ndef collate_fn(examples):\n    instances = []\n    for example in examples:\n        prompt = example[\"text prompt\"]\n\n        user_content = [{\"type\": \"text\", \"text\": \"Caption the video.\"}]\n        user_content.append({\"type\": \"video\", \"path\": example[\"video link\"]})\n\n        messages = [\n            {\"role\": \"user\", \"content\": user_content},\n            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": f\"{prompt}\"}]}\n        ]\n\n        instance = processor.apply_chat_template(messages, add_generation_prompt=False,\n                                                 tokenize=True, return_dict=True, return_tensors=\"pt\").to(\"cuda\").to(model.dtype)\n        instances.append(instance)\n\n\n    input_ids = pad_sequence(\n        [inst[\"input_ids\"].squeeze(0) for inst in instances],\n        batch_first=True,\n        padding_value=processor.tokenizer.pad_token_id\n    )\n    attention_mask = pad_sequence(\n        [inst[\"attention_mask\"].squeeze(0) for inst in instances],\n        batch_first=True,\n        padding_value=0\n    )\n    labels = pad_sequence(\n        [inst[\"input_ids\"].squeeze(0).clone() for inst in instances],\n        batch_first=True,\n        padding_value=-100\n    )\n\n    labels[labels == image_token_id] = -100\n\n    out = {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\n\n    # Step 1: figure out maximum frames, height, width across the batch\n    pvs = [inst[\"pixel_values\"].squeeze(0) for inst in instances if \"pixel_values\" in inst]\n    if pvs:  # there is at least one non-None pixel_values\n        max_frames = max(pv.shape[0] for pv in pvs)\n        max_h = max(pv.shape[-2] for pv in pvs)\n        max_w = max(pv.shape[-1] for pv in pvs)\n    else:\n        max_h = max_w = processor.video_size['longest_edge']\n        max_frames = 1\n\n    padded_pixel_values_list = []\n    for ex in instances:\n        pv = ex.get(\"pixel_values\", None).squeeze(0)\n\n        if pv is None:\n            # text-only => fill pixel data + mask with zeros\n            shape_pv = (max_frames, 3, max_h, max_w)\n            padded_pv = torch.zeros(shape_pv, dtype=torch.float32)\n        else:\n            f, c, h, w = pv.shape\n            # Prepare final storage\n            padded_pv = torch.zeros(\n                (max_frames, c, max_h, max_w),\n                dtype=pv.dtype,\n                device=pv.device\n            )\n            padded_pv[:f, :, :h, :w] = pv\n        padded_pixel_values_list.append(padded_pv)\n\n    out[\"pixel_values\"] = torch.stack(padded_pixel_values_list, dim=0)\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:10:26.601890Z","iopub.execute_input":"2025-03-01T15:10:26.602264Z","iopub.status.idle":"2025-03-01T15:10:26.612763Z","shell.execute_reply.started":"2025-03-01T15:10:26.602236Z","shell.execute_reply":"2025-03-01T15:10:26.612142Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nmodel_name = model_id.split(\"/\")[-1]\n\ntraining_args = TrainingArguments(\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    warmup_steps=50,\n    learning_rate=1e-4,\n    weight_decay=0.01,\n    logging_steps=25,\n    save_strategy=\"steps\",\n    save_steps=250,\n    save_total_limit=1,\n    optim=\"adamw_hf\", # for 8-bit, keep paged_adamw_8bit, else adamw_hf\n    bf16=True,\n    output_dir=f\"./{model_name}-video-feedback\",\n    remove_unused_columns=False,\n    report_to=\"wandb\",\n    dataloader_pin_memory=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:10:33.002553Z","iopub.execute_input":"2025-03-01T15:10:33.002864Z","iopub.status.idle":"2025-03-01T15:10:33.257808Z","shell.execute_reply.started":"2025-03-01T15:10:33.002839Z","shell.execute_reply":"2025-03-01T15:10:33.257140Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=train_ds,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:10:33.639328Z","iopub.execute_input":"2025-03-01T15:10:33.639588Z","iopub.status.idle":"2025-03-01T15:10:33.655361Z","shell.execute_reply.started":"2025-03-01T15:10:33.639569Z","shell.execute_reply":"2025-03-01T15:10:33.654528Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:10:35.788939Z","iopub.execute_input":"2025-03-01T15:10:35.789278Z","iopub.status.idle":"2025-03-01T18:22:48.481355Z","shell.execute_reply.started":"2025-03-01T15:10:35.789248Z","shell.execute_reply":"2025-03-01T18:22:48.480526Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 3:12:05, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>3.795700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.246900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.338300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.260700</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.247400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.254300</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.212300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.296600</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.247900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.266600</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.235600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.237100</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.207300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.212900</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.226100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.221700</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.243400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.237700</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.215600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.277100</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.272700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.234100</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.260500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.203500</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.204900</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.256300</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>0.232900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.222000</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>0.234600</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.234600</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>0.197000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.250200</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>0.219100</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.207800</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>0.214200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.239900</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>0.193100</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.241200</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>0.231200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.242700</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>0.236200</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.234900</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>0.221000</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.230400</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>0.229300</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.255500</td>\n    </tr>\n    <tr>\n      <td>1175</td>\n      <td>0.206800</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.199600</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>0.277200</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.212900</td>\n    </tr>\n    <tr>\n      <td>1275</td>\n      <td>0.257000</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.206900</td>\n    </tr>\n    <tr>\n      <td>1325</td>\n      <td>0.199700</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.211500</td>\n    </tr>\n    <tr>\n      <td>1375</td>\n      <td>0.208100</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.208500</td>\n    </tr>\n    <tr>\n      <td>1425</td>\n      <td>0.184900</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.213700</td>\n    </tr>\n    <tr>\n      <td>1475</td>\n      <td>0.220900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.249800</td>\n    </tr>\n    <tr>\n      <td>1525</td>\n      <td>0.193900</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.230900</td>\n    </tr>\n    <tr>\n      <td>1575</td>\n      <td>0.231900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.231400</td>\n    </tr>\n    <tr>\n      <td>1625</td>\n      <td>0.221800</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.226600</td>\n    </tr>\n    <tr>\n      <td>1675</td>\n      <td>0.212800</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.185000</td>\n    </tr>\n    <tr>\n      <td>1725</td>\n      <td>0.192500</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.194100</td>\n    </tr>\n    <tr>\n      <td>1775</td>\n      <td>0.193700</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.212700</td>\n    </tr>\n    <tr>\n      <td>1825</td>\n      <td>0.178000</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.227000</td>\n    </tr>\n    <tr>\n      <td>1875</td>\n      <td>0.188100</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.230900</td>\n    </tr>\n    <tr>\n      <td>1925</td>\n      <td>0.212600</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.256400</td>\n    </tr>\n    <tr>\n      <td>1975</td>\n      <td>0.182000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.221800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2000, training_loss=0.2845357277393341, metrics={'train_runtime': 11532.2088, 'train_samples_per_second': 0.173, 'train_steps_per_second': 0.173, 'total_flos': 7980278517236160.0, 'train_loss': 0.2845357277393341, 'epoch': 1.0})"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"messages = [{\"role\": \"user\",\n                 \"content\": [{\"type\": \"text\", \"text\": \"Describe the video in details.\"},\n                  {\"type\": \"video\", \"path\": \"https://huggingface.co/datasets/hexuan21/VideoFeedback-videos-mp4/resolve/main/p/p000304.mp4\"}]}]\n\n\ninputs = processor.apply_chat_template(messages, add_generation_prompt=True,\n                                          tokenize=True, return_dict=True, return_tensors=\"pt\").to(\"cuda\").to(model.dtype)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\n\nprint(generated_texts[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T18:22:48.482690Z","iopub.execute_input":"2025-03-01T18:22:48.482948Z","iopub.status.idle":"2025-03-01T18:22:56.502076Z","shell.execute_reply.started":"2025-03-01T18:22:48.482925Z","shell.execute_reply":"2025-03-01T18:22:56.501321Z"}},"outputs":[{"name":"stdout","text":"User: Describe the video in details.You are provided the following series of three frames from a 0:00:03 [H:MM:SS] video.\n\nFrame from 00:00:\nFrame from 00:01:\nFrame from 00:02:\n\n\nAssistant: a woman in a white shirt walks up to the podium.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}